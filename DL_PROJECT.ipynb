{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing Relevant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AGnXgbdFbBqK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model, save_model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Conv2D, MaxPooling2D"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use Pre Trained Face Detector to Crop Images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed image: 86717448_2419567865022562_3625521906192482304_n.jpg\n"
          ]
        }
      ],
      "source": [
        "def resize_images_with_face_focus(folder_path, output_folder, target_size=(300, 300)):\n",
        "    # Load face detection model\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Iterate over the images in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            # Read the image\n",
        "            image = cv2.imread(image_path)\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Detect faces in the image\n",
        "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "            if len(faces) > 0:\n",
        "                # Select the largest face\n",
        "                (x, y, w, h) = max(faces, key=lambda rect: rect[2] * rect[3])\n",
        "\n",
        "                # Calculate the center coordinates of the face\n",
        "                center_x = x + w // 2\n",
        "                center_y = y + h // 2\n",
        "\n",
        "                # Calculate the size of the square region around the face\n",
        "                face_size = max(w, h)\n",
        "\n",
        "                # Calculate the region of interest (ROI) coordinates\n",
        "                roi_x = center_x - face_size // 2\n",
        "                roi_y = center_y - face_size // 2\n",
        "\n",
        "                # Extract the region of interest (ROI)\n",
        "                roi = image[roi_y:roi_y + face_size, roi_x:roi_x + face_size]\n",
        "\n",
        "                # Resize the ROI to the target size\n",
        "                resized_roi = cv2.resize(roi, target_size)\n",
        "\n",
        "                # Save the resized image\n",
        "                cv2.imwrite(output_path, resized_roi)\n",
        "\n",
        "                print(f\"Processed image: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"No faces found in the image: {filename}\")\n",
        "\n",
        "# Example usage\n",
        "folder_path = 'Old'\n",
        "output_folder = 'New'\n",
        "resize_images_with_face_focus(folder_path, output_folder)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Face Embeddings Tenserflow Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 3s 332ms/step\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 3s 574ms/step - loss: 0.1878 - val_loss: 0.1061\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 2s 537ms/step - loss: 0.0640 - val_loss: 0.0301\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 2s 517ms/step - loss: 0.0302 - val_loss: 0.0264\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 2s 538ms/step - loss: 0.0286 - val_loss: 0.0257\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 2s 476ms/step - loss: 0.0226 - val_loss: 0.0194\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.0204 - val_loss: 0.0183\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 3s 699ms/step - loss: 0.0198 - val_loss: 0.0182\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 2s 566ms/step - loss: 0.0187 - val_loss: 0.0169\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 2s 493ms/step - loss: 0.0182 - val_loss: 0.0175\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.0177 - val_loss: 0.0163\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 2s 550ms/step - loss: 0.0171 - val_loss: 0.0159\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 2s 536ms/step - loss: 0.0169 - val_loss: 0.0158\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 2s 550ms/step - loss: 0.0165 - val_loss: 0.0154\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 2s 563ms/step - loss: 0.0163 - val_loss: 0.0154\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 2s 532ms/step - loss: 0.0161 - val_loss: 0.0151\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 2s 562ms/step - loss: 0.0159 - val_loss: 0.0150\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 2s 617ms/step - loss: 0.0157 - val_loss: 0.0147\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 2s 576ms/step - loss: 0.0154 - val_loss: 0.0146\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 2s 505ms/step - loss: 0.0152 - val_loss: 0.0145\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 2s 476ms/step - loss: 0.0150 - val_loss: 0.0142\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 2s 478ms/step - loss: 0.0149 - val_loss: 0.0142\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 2s 511ms/step - loss: 0.0147 - val_loss: 0.0143\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.0144 - val_loss: 0.0139\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 2s 558ms/step - loss: 0.0145 - val_loss: 0.0143\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 2s 620ms/step - loss: 0.0149 - val_loss: 0.0140\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 2s 484ms/step - loss: 0.0146 - val_loss: 0.0147\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 2s 546ms/step - loss: 0.0146 - val_loss: 0.0143\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 2s 467ms/step - loss: 0.0143 - val_loss: 0.0143\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.0142 - val_loss: 0.0137\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.0141 - val_loss: 0.0138\n"
          ]
        }
      ],
      "source": [
        "# Path to the dataset\n",
        "dataset_path = \"Images\"\n",
        "\n",
        "def load_dataset(dataset_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for label in os.listdir(dataset_path):\n",
        "        label_path = os.path.join(dataset_path, label)\n",
        "        for image_file in os.listdir(label_path):\n",
        "            image_path = os.path.join(label_path, image_file)\n",
        "            image = load_img(image_path, target_size=(64, 64))\n",
        "            image = img_to_array(image)\n",
        "            data.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "    data = np.array(data, dtype=\"float32\")\n",
        "    labels = np.array(labels)\n",
        "    return data, labels\n",
        "\n",
        "data, labels = load_dataset(dataset_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the input data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Define the embedding model\n",
        "embedding_dim = 2048\n",
        "embedding_model = tf.keras.models.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(embedding_dim, activation='relu'),\n",
        "])\n",
        "\n",
        "data, labels = load_dataset(dataset_path)\n",
        "\n",
        "# Normalize the input data\n",
        "data = data / 255.0\n",
        "\n",
        "# Load the pre-trained model for calculating embeddings\n",
        "pretrained_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(64, 64, 3),\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "# Calculate embeddings for all the images\n",
        "embeddings = pretrained_model.predict(data)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train2, x_val, y_train2, y_val = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compile the embedding model\n",
        "embedding_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the embedding model\n",
        "embedding_model.fit(x_train, x_train2, epochs=30, validation_data=(x_test, x_val))\n",
        "\n",
        "# Save the embedding model\n",
        "embedding_model.save('face_embedding.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download Pre Trained Detection and Recognition Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr69cv8YbHT2"
      },
      "outputs": [],
      "source": [
        "# Download the shape predictor model if not already downloaded\n",
        "shape_predictor_file = 'shape_predictor_68_face_landmarks.dat'\n",
        "if not os.path.exists(shape_predictor_file):\n",
        "    print(\"Downloading shape predictor model...\")\n",
        "    urllib.request.urlretrieve(\"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\",\n",
        "                               \"shape_predictor_68_face_landmarks.dat.bz2\")\n",
        "    print(\"Extracting shape predictor model...\")\n",
        "    os.system(\"bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Face Detector and Recognizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iCpvE-1sbPiG"
      },
      "outputs": [],
      "source": [
        "# Set up face detector and face recognition models\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "shape_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "# Load the TensorFlow face recognition model\n",
        "model_file = 'face_embedding.h5'\n",
        "face_recognizer = tf.keras.models.load_model(model_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the folder containing labeled images\n",
        "data_folder = 'Images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PYiWzVebDgb",
        "outputId": "8a0def19-4a89-4435-ad58-5fb3e27e297e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the folder containing labeled images\n",
        "data_folder = 'drive/MyDrive/Images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n148fq9vbXfs"
      },
      "outputs": [],
      "source": [
        "# Load the labeled images and their corresponding names\n",
        "labeled_images = []\n",
        "labels = []\n",
        "for label_name in os.listdir(data_folder):\n",
        "    label_folder = os.path.join(data_folder, label_name)\n",
        "    for image_name in os.listdir(label_folder):\n",
        "        image_path = os.path.join(label_folder, image_name)\n",
        "        labeled_images.append(cv2.imread(image_path))\n",
        "        labels.append(label_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating Embeddings for the Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "da5TXZGKbZ5O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n"
          ]
        }
      ],
      "source": [
        "# Calculate face embeddings for the labeled images\n",
        "embeddings = []\n",
        "for image in labeled_images:\n",
        "    faces = detector(image)\n",
        "    for face in faces:\n",
        "        shape = shape_predictor(image, face)\n",
        "        (x, y, w, h) = (face.left(), face.top(), face.width(), face.height())\n",
        "        face_region = image[y:y+h, x:x+w]  # Extract face region\n",
        "        face_region = cv2.resize(face_region, (64, 64))  # Resize to match model input size\n",
        "        face_region = np.expand_dims(face_region, axis=0)  # Add batch dimension\n",
        "        embedding = face_recognizer.predict(face_region)[0]\n",
        "        embeddings.append(embedding)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating/Accessing The Attendance CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up attendance CSV file\n",
        "csv_file = 'attendance.csv'\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Name', 'Attendance', 'Time'])  # Updated header row\n",
        "        unique_labels = list(set(labels))  # Get unique labels\n",
        "        for label_name in unique_labels:\n",
        "            writer.writerow([label_name, 'Absent', ''])  # Write unique label names initially with \"Absent\" status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA8oQvP5dgBO",
        "outputId": "3d22b74a-92ad-4a22-e95d-41974e5cdb2f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up attendance CSV file in Google Drive\n",
        "csv_file = '/content/drive/MyDrive/attendance.csv'\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Name', 'Timestamp'])\n",
        "        print(\"Attendance CSV file created.\")\n",
        "else:\n",
        "    print(\"Attendance CSV file already exists.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Detection using Device Camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n"
          ]
        }
      ],
      "source": [
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "attendance_data = {}  # Store attendance data\n",
        "last_label = None  # Store the last predicted label\n",
        "stable_count = 0  # Count the number of stable predictions\n",
        "\n",
        "# Load existing attendance data from CSV\n",
        "with open(csv_file, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    header = next(reader)  # Get header row\n",
        "    for row in reader:\n",
        "        label = row[0]\n",
        "        attendance = row[1]\n",
        "        attendance_data[label] = attendance\n",
        "\n",
        "while True:\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Convert the frame to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the grayscale frame\n",
        "    faces = detector(gray)\n",
        "\n",
        "    # Iterate over detected faces\n",
        "    for face in faces:\n",
        "        # Determine face landmarks for alignment\n",
        "        shape = shape_predictor(frame, face)\n",
        "        (x, y, w, h) = (face.left(), face.top(), face.width(), face.height())\n",
        "        face_region = frame[y:y+h, x:x+w]  # Extract face region\n",
        "        face_region = cv2.resize(face_region, (64, 64))  # Resize to match model input size\n",
        "        face_region = np.expand_dims(face_region, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Calculate face embedding for recognition\n",
        "        embedding = face_recognizer.predict(face_region)[0]\n",
        "\n",
        "        # Compare face embedding with labeled embeddings\n",
        "        distances = np.linalg.norm(embeddings - np.expand_dims(embedding, axis=0), axis=1)\n",
        "        min_distance_index = np.argmin(distances)\n",
        "        min_distance = distances[min_distance_index]\n",
        "\n",
        "        # Threshold for face recognition\n",
        "        if min_distance < 150:\n",
        "            label = labels[min_distance_index]\n",
        "\n",
        "            # Check if the label exists in the attendance data\n",
        "            if label in attendance_data:\n",
        "                if last_label == label:\n",
        "                    stable_count += 1\n",
        "                    if stable_count == 10:\n",
        "                        if attendance_data[label] == 'Present':\n",
        "                            label = f'{label} (Already Marked)'\n",
        "                            cv2.putText(frame, 'Marked', (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "                        else:\n",
        "                            # Update attendance data to mark as Present\n",
        "                            attendance_data[label] = 'Present'\n",
        "                            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                            attendance_data[label] = 'Present'\n",
        "                else:\n",
        "                    last_label = label\n",
        "                    stable_count = 0\n",
        "\n",
        "            # Display recognized label on the frame\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow('Attendance System', frame)\n",
        "\n",
        "    # Exit on 'q' key press\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Update attendance\n",
        "with open(csv_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)  # Write the header row\n",
        "    for label, attendance in attendance_data.items():\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        if attendance == 'Present':\n",
        "            writer.writerow([label, attendance, timestamp])  # Write the updated attendance data\n",
        "        else:\n",
        "            writer.writerow([label, attendance, ''])  # Write the absent attendance data\n",
        "\n",
        "# Release the video capture and close windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
